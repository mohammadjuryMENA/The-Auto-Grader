# The Auto-Grader: Judge Model Training Pipeline

A complete end-to-end pipeline for training a specialized "Judge Model" that can evaluate AI model responses based on rubrics using models under 3B parameters.

## ğŸ¯ Project Overview

This project trains a small language model (Qwen-2.5-1.5B-Instruct) to act as a judge, evaluating AI responses against specific rubrics and providing structured scores (1-5) with reasoning.

## ğŸ—ï¸ Project Structure

```
The-Auto-Grader/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generate_dataset.py      # Dataset generation script
â”‚   â”œâ”€â”€ train_dataset.json       # Generated training data
â”‚   â””â”€â”€ test_cases.json          # Test cases for evaluation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                 # Model training script
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation script
â”‚   â””â”€â”€ inference.py             # Inference utilities
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ auto_grader_colab.ipynb  # Google Colab notebook
â”œâ”€â”€ results/
â”‚   â””â”€â”€ evaluation_results.json  # Saved evaluation results
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Platform Compatibility

- âœ… **Linux + CUDA GPU**: Optimal (4-bit quantization, ~20 min training)
- âœ… **Google Colab**: Optimal (Free T4 GPU, recommended for all users)
- âš ï¸ **macOS**: Limited (no quantization, slower training)
- âš ï¸ **CPU only**: Very slow (not recommended)

> **macOS Users**: See [MACOS_TRAINING_GUIDE.md](MACOS_TRAINING_GUIDE.md) for important information and workarounds.

### 1. Installation

```bash
pip install -r requirements.txt
```

### 2. Generate Training Dataset

```bash
python data/generate_dataset.py
```

This creates a balanced dataset with equal distribution of scores (1-5) to avoid the "Lazy Judge" problem.

### 3. Train the Judge Model

```bash
python src/train.py
```

Or use the Google Colab notebook for free GPU access.

### 4. Evaluate the Model

```bash
python src/evaluate.py
```

## ğŸ“Š The Three Challenge Levels

### Level 1: The Basics âœ…

- **Test 1 (Math)**: Detect "2+2=5" is wrong â†’ Score: 1
- **Test 2 (Hallucination)**: Detect "Elon Musk is CEO of Apple" is false â†’ Score: 1

### Level 2: The Stress Test ğŸ”¥

- **The Over-Refusal Trap**: Judge must recognize that refusing to help with "kill a process in Linux" is unhelpful â†’ Score: 1

### Level 3: The Bonus Challenge ğŸ’

- **Jailbreak Resistance**: Judge must resist prompt injection attacks and maintain evaluation integrity â†’ Score: 1

## ğŸ”§ Technical Details

- **Base Model**: Qwen/Qwen2.5-1.5B-Instruct (1.5B parameters)
- **Method**: Supervised Fine-Tuning (SFT)
- **Framework**: HuggingFace TRL
- **Quantization**: 4-bit for efficient training
- **Max Sequence Length**: 1024 tokens

## ğŸ’¡ Key Engineering Solutions

### 1. Class Balance

The dataset ensures equal distribution of scores (1-5) to prevent the model from always predicting the majority class.

### 2. Consistency

Training data is carefully curated to avoid contradictory scores for similar errors.

### 3. Context-Aware Grading

Model is trained to understand context (e.g., "kill" in technical vs harmful contexts).

### 4. Robustness

Includes adversarial examples to resist prompt injection attacks.

## ğŸ“ˆ Results

After training, the model achieves:

- âœ… 100% accuracy on Level 1 (Basic tests)
- âœ… High accuracy on Level 2 (Context-aware grading)
- âœ… Resistance to Level 3 (Jailbreak attempts)

## ğŸ¥ Demo Video

[Link to 3-minute demo video showing model behavior]

## ğŸ“ Sample Output

```json
{
  "score": 1,
  "reasoning": "The response provides completely incorrect mathematical information. 2+2 equals 4, not 5. This is a fundamental error in basic arithmetic."
}
```

## ğŸ¤ Contributing

This project is part of the MENA Devs Competition - Track B.

## ğŸ“„ License

MIT License
